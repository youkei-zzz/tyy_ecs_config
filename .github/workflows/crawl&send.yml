name: Deploy crawler

on:
  workflow_dispatch:
  push:
    branches: [ "main" ]
  schedule:
    # Run every day at 02:00 UTC (10:00 Beijing time)
    - cron: '0 2 * * *'

env:
  CRAWLER_SCRIPT: "完整爬取脚本.js"

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 9

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install

      - name: Install Playwright browsers
        run: npx playwright install --with-deps

      - name: Run crawler
        id: run_crawler
        run: |
          set -o pipefail
          start_time=$(date +%s)
          node "$CRAWLER_SCRIPT" 2>&1 | tee crawl-output.log
          exit_code=${PIPESTATUS[0]}
          end_time=$(date +%s)
          duration=$((end_time - start_time))
          echo "exit_code=$exit_code" >> "$GITHUB_OUTPUT"
          echo "duration=$duration" >> "$GITHUB_OUTPUT"
          exit $exit_code

      - name: Commit and push updated dataset
        if: success()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/ecs-zones-metadata.json data/ecs-zones-simple.json || true
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi
          git commit -m "Update ECS zones dataset [skip ci]"
          git push

      - name: Generate execution summary
        id: summary
        if: always()
        run: |
          set -euo pipefail
          exit_code="${{ steps.run_crawler.outputs.exit_code }}"
          duration="${{ steps.run_crawler.outputs.duration }}"
          if [ "$exit_code" == "0" ]; then
            status="SUCCESS"
          else
            status="FAILURE"
          fi
          hours=$((duration / 3600))
          minutes=$(((duration % 3600) / 60))
          seconds=$((duration % 60))
          if [ $hours -gt 0 ]; then
            time_str="${hours}h ${minutes}m ${seconds}s"
          elif [ $minutes -gt 0 ]; then
            time_str="${minutes}m ${seconds}s"
          else
            time_str="${seconds}s"
          fi
          cat > summary.txt <<EOF
          ====================================
          ECS config crawler execution report
          ====================================
          Status: ${status}
          Duration: ${time_str}
          Time (Asia/Shanghai): $(TZ='Asia/Shanghai' date '+%Y-%m-%d %H:%M:%S')
          Dataset files: data/ecs-zones-metadata.json, data/ecs-zones-simple.json
          ====================================
          EOF
          echo "summary_file=summary.txt" >> "$GITHUB_OUTPUT"

      - name: Package region folders
        id: package_outputs
        if: always()
        run: |
          set -euo pipefail
          archive_path="region-configs.tar.gz"
          config_list=$(mktemp)
          find . -type f \( -name 'CPU-可选项.txt' -o -name '内存-可选项.txt' \) -print0 > "$config_list"
          region_dirs_tmp=$(mktemp)
          if [ -s "$config_list" ]; then
            while IFS= read -r -d '' file; do
              rel="${file#./}"
              top="${rel%%/*}"
              case "$top" in
                ""|".git"|".github"|"node_modules")
                  continue
                  ;;
              esac
              printf '%s\0' "$top" >> "$region_dirs_tmp"
            done < "$config_list"
          fi
          if [ -s "$region_dirs_tmp" ]; then
            unique_dirs=$(mktemp)
            sort -zu "$region_dirs_tmp" > "$unique_dirs"
            mapfile -d '' -t region_dirs < "$unique_dirs"
            tar -czf "$archive_path" "${region_dirs[@]}"
            echo "archive_created=true" >> "$GITHUB_OUTPUT"
            echo "archive_path=$archive_path" >> "$GITHUB_OUTPUT"
            printf 'attachment_list<<EOF\n%s\nEOF\n' "$archive_path" >> "$GITHUB_OUTPUT"
          else
            echo "archive_created=false" >> "$GITHUB_OUTPUT"
            printf 'attachment_list<<EOF\n\nEOF\n' >> "$GITHUB_OUTPUT"
          fi

      - name: Upload crawler log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: crawl-output
          path: crawl-output.log
          if-no-files-found: ignore

      - name: Upload region config archive
        if: always() && steps.package_outputs.outputs.archive_created == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: region-configs
          path: ${{ steps.package_outputs.outputs.archive_path }}
          if-no-files-found: error
