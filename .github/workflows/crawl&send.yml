name: Deploy crawler

on:
  workflow_dispatch:
  push:
    branches: [ "main" ]
  schedule:
    # Trigger every 12 hours. A guard step below makes sure the crawler itself only
    # runs if at least 36 hours passed since the previous completed execution.
    - cron: '0 */24 * * *'

env:
  RUN_INTERVAL_MINUTES: "2160" # 36 hours
  CRAWLER_SCRIPT: "完整爬取脚本.js"

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Decide whether to run crawler
        id: interval
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const requiredMinutes = Number(process.env.RUN_INTERVAL_MINUTES ?? 2160);
            if (context.eventName !== 'schedule') {
              core.setOutput('ready', 'true');
              core.info(`Event "${context.eventName}" triggered the workflow -> running immediately.`);
              return;
            }

            const minSpacingMs = requiredMinutes * 60 * 1000;
            const { data } = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: context.workflow,
              status: 'completed',
              per_page: 5,
            });

            const previous = data.workflow_runs.find((run) => run.id !== context.runId && run.conclusion);
            if (!previous) {
              core.setOutput('ready', 'true');
              core.info('No prior completed crawler run found, running now.');
              return;
            }

            const elapsedMs = Date.now() - Date.parse(previous.updated_at);
            if (elapsedMs >= minSpacingMs) {
              core.setOutput('ready', 'true');
              core.info(`Last completed run was ${(elapsedMs / 3600000).toFixed(1)} hours ago.`);
            } else {
              core.setOutput('ready', 'false');
              const remaining = ((minSpacingMs - elapsedMs) / 3600000).toFixed(1);
              core.notice(`Only ${(elapsedMs / 3600000).toFixed(1)} hours passed since the last run. Waiting another ${remaining} hours.`);
            }

      - name: Skip because interval not reached
        if: steps.interval.outputs.ready != 'true'
        run: echo "36-hour interval has not passed yet, skipping crawler execution for this run."

      - name: Checkout repository
        if: steps.interval.outputs.ready == 'true'
        uses: actions/checkout@v4

      - name: Setup Node.js
        if: steps.interval.outputs.ready == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
          cache: npm

      - name: Install dependencies
        if: steps.interval.outputs.ready == 'true'
        run: npm ci

      - name: Install Playwright browsers
        if: steps.interval.outputs.ready == 'true'
        run: npx playwright install --with-deps

      - name: Run crawler
        id: run_crawler
        if: steps.interval.outputs.ready == 'true'
        run: |
          set -o pipefail
          node "$CRAWLER_SCRIPT" 2>&1 | tee crawl-output.log

      - name: Package province folders
        id: package_outputs
        if: ${{ always() && steps.interval.outputs.ready == 'true' }}
        run: |
          set -euo pipefail
          archive_path="region-configs.tar.gz"
          config_list=$(mktemp)
          find . -type f \( -name 'CPU-可选项.txt' -o -name '内存-可选项.txt' \) -print0 > "$config_list"

          region_dirs_tmp=$(mktemp)
          if [ -s "$config_list" ]; then
            while IFS= read -r -d '' file; do
              rel="${file#./}"
              top="${rel%%/*}"
              case "$top" in
                ""|".git"|".github"|"node_modules")
                  continue
                  ;;
              esac
              printf '%s\0' "$top" >> "$region_dirs_tmp"
            done < "$config_list"
          fi

          if [ -s "$region_dirs_tmp" ]; then
            unique_dirs=$(mktemp)
            sort -zu "$region_dirs_tmp" > "$unique_dirs"
            mapfile -d '' -t region_dirs < "$unique_dirs"
            tar -czf "$archive_path" "${region_dirs[@]}"
            echo "archive_created=true" >> "$GITHUB_OUTPUT"
            echo "archive_path=$archive_path" >> "$GITHUB_OUTPUT"
            printf 'attachment_list<<EOF\n%s\nEOF\n' "$archive_path" >> "$GITHUB_OUTPUT"
          else
            echo "archive_created=false" >> "$GITHUB_OUTPUT"
            printf 'attachment_list<<EOF\n\nEOF\n' >> "$GITHUB_OUTPUT"
          fi

      - name: Resolve sender email
        id: sender
        if: ${{ steps.interval.outputs.ready == 'true' }}
        env:
          SMTP_FROM_SECRET: ${{ secrets.SMTP_FROM }}
          SMTP_USERNAME: ${{ secrets.SMTP_USERNAME }}
        run: |
          resolved="$SMTP_FROM_SECRET"
          if [ -z "$resolved" ]; then
            resolved="$SMTP_USERNAME"
          fi
          if [ -z "$resolved" ]; then
            echo "No SMTP sender available." >&2
            exit 1
          fi
          echo "value=$resolved" >> "$GITHUB_OUTPUT"
          echo "EMAIL_FROM=$resolved" >> "$GITHUB_ENV"

      - name: Resolve SMTP secure mode
        id: smtp_secure
        if: ${{ steps.interval.outputs.ready == 'true' }}
        env:
          SMTP_SECURE_SECRET: ${{ secrets.SMTP_SECURE }}
        run: |
          value="${SMTP_SECURE_SECRET,,}"
          case "$value" in
            ""|"auto"|"default")
              resolved="true"
              ;;
            "true"|"1"|"ssl"|"tls")
              resolved="true"
              ;;
            "false"|"0"|"starttls"|"plain")
              resolved="false"
              ;;
            *)
              resolved="true"
              ;;
          esac
          echo "value=$resolved" >> "$GITHUB_OUTPUT"

      - name: Upload crawler log
        if: ${{ steps.interval.outputs.ready == 'true' && always() }}
        uses: actions/upload-artifact@v4
        with:
          name: crawl-output
          path: crawl-output.log
          if-no-files-found: ignore

      - name: Upload region config archive
        if: ${{ steps.interval.outputs.ready == 'true' && always() && steps.package_outputs.outputs.archive_created == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: region-configs
          path: ${{ steps.package_outputs.outputs.archive_path }}
          if-no-files-found: error

      - name: Email crawl report
        if: ${{ steps.interval.outputs.ready == 'true' && always() }}
        uses: dawidd6/action-send-mail@v4
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "自动化-爬取云主机参数信息"
          to: ${{ secrets.NOTIFY_EMAIL }}
          from: ${{ env.EMAIL_FROM }}
          secure: ${{ steps.smtp_secure.outputs.value == 'true' }}
          body: file://crawl-output.log
          attachments: ${{ steps.package_outputs.outputs.attachment_list }}
