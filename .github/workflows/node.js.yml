name: Deploy crawler

on:
  workflow_dispatch:
  push:
    branches: [ "main" ]
  schedule:
    # Trigger every 12 hours. A guard step below makes sure the crawler itself only
    # runs if at least 36 hours passed since the previous completed execution.
    - cron: '0 */12 * * *'

env:
  RUN_INTERVAL_MINUTES: "2160" # 36 hours
  CRAWLER_SCRIPT: "完整爬取脚本.js"

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: Decide whether to run crawler
        id: interval
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const requiredMinutes = Number(process.env.RUN_INTERVAL_MINUTES ?? 2160);
            if (context.eventName !== 'schedule') {
              core.setOutput('ready', 'true');
              core.info(`Event "${context.eventName}" triggered the workflow -> running immediately.`);
              return;
            }

            const minSpacingMs = requiredMinutes * 60 * 1000;
            const { data } = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: context.workflow,
              status: 'completed',
              per_page: 5,
            });

            const previous = data.workflow_runs.find((run) => run.id !== context.runId && run.conclusion);
            if (!previous) {
              core.setOutput('ready', 'true');
              core.info('No prior completed crawler run found, running now.');
              return;
            }

            const elapsedMs = Date.now() - Date.parse(previous.updated_at);
            if (elapsedMs >= minSpacingMs) {
              core.setOutput('ready', 'true');
              core.info(`Last completed run was ${(elapsedMs / 3600000).toFixed(1)} hours ago.`);
            } else {
              core.setOutput('ready', 'false');
              const remaining = ((minSpacingMs - elapsedMs) / 3600000).toFixed(1);
              core.notice(`Only ${(elapsedMs / 3600000).toFixed(1)} hours passed since the last run. Waiting another ${remaining} hours.`);
            }

      - name: Skip because interval not reached
        if: steps.interval.outputs.ready != 'true'
        run: echo "36-hour interval has not passed yet, skipping crawler execution for this run."

      - name: Checkout repository
        if: steps.interval.outputs.ready == 'true'
        uses: actions/checkout@v4

      - name: Setup Node.js
        if: steps.interval.outputs.ready == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
          cache: npm

      - name: Install dependencies
        if: steps.interval.outputs.ready == 'true'
        run: npm ci

      - name: Install Playwright browsers
        if: steps.interval.outputs.ready == 'true'
        run: npx playwright install --with-deps

      - name: Run crawler
        id: run_crawler
        if: steps.interval.outputs.ready == 'true'
        run: |
          set -o pipefail
          node "$CRAWLER_SCRIPT" 2>&1 | tee crawl-output.log

      - name: Package generated files
        id: package_outputs
        if: ${{ always() && steps.interval.outputs.ready == 'true' }}
        run: |
          if [ ! -f crawl-output.log ]; then
            touch crawl-output.log
          fi

          attachments="crawl-output.log"
          list_file=$(mktemp)
          git ls-files -io --exclude-standard > "$list_file" || true

          filtered_file=$(mktemp)
          if [ -s "$list_file" ]; then
            while IFS= read -r item; do
              [ -z "$item" ] && continue
              case "$item" in
                node_modules/*|tmp.js)
                  continue
                  ;;
              esac
              printf '%s\n' "$item" >> "$filtered_file"
            done < "$list_file"
          fi

          if [ -s "$filtered_file" ]; then
            tar -czf crawler-generated.tar.gz --files-from "$filtered_file"
            attachments="$attachments"$'\n'"crawler-generated.tar.gz"
            echo "archive_created=true" >> "$GITHUB_OUTPUT"
            echo "archive_path=crawler-generated.tar.gz" >> "$GITHUB_OUTPUT"
          else
            echo "archive_created=false" >> "$GITHUB_OUTPUT"
          fi

          printf 'attachment_list<<EOF\n%s\nEOF\n' "$attachments" >> "$GITHUB_OUTPUT"

      - name: Resolve sender email
        id: sender
        if: ${{ steps.interval.outputs.ready == 'true' }}
        env:
          SMTP_FROM: ${{ secrets.SMTP_FROM }}
          SMTP_USERNAME: ${{ secrets.SMTP_USERNAME }}
        run: |
          if [ -n "$SMTP_FROM" ]; then
            echo "value=$SMTP_FROM" >> "$GITHUB_OUTPUT"
          else
            echo "value=$SMTP_USERNAME" >> "$GITHUB_OUTPUT"
          fi

      - name: Upload crawler log
        if: ${{ steps.interval.outputs.ready == 'true' && always() }}
        uses: actions/upload-artifact@v4
        with:
          name: crawl-output
          path: crawl-output.log
          if-no-files-found: ignore

      - name: Email crawl report
        if: ${{ steps.interval.outputs.ready == 'true' && always() }}
        uses: dawidd6/action-send-mail@v4
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "CTYun crawler result - ${{ steps.run_crawler.outcome }} (run #${{ github.run_number }})"
          to: ${{ secrets.NOTIFY_EMAIL }}
          from: ${{ steps.sender.outputs.value }}
          secure: true
          body: file://crawl-output.log
          attachments: ${{ steps.package_outputs.outputs.attachment_list }}
